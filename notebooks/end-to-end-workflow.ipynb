{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:05.654901Z",
     "start_time": "2024-06-14T11:31:05.649760Z"
    }
   },
   "source": [
    "from perturbench.dataset import PerturbationDataset\n",
    "from perturbench.scenarios import RandomSplitScenario\n",
    "from perturbench.models import RandomModel\n",
    "from perturbench.metrics import AverageDifferenceMetric\n",
    "from perturbench.benchmark import PerturbationBenchmark\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:44:52.587907Z",
     "start_time": "2024-06-14T09:44:52.529586Z"
    }
   },
   "source": "# End-to-end pipeline for benchmarking perturbation predictions using perturbench"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare dataset\n",
    "\n",
    "We will prepare an example dataset with 3 genetic perturbations, and a control condition. 6,000 cells total, with sex as a covariate. We could also use a preprepared dataset."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:11.061682Z",
     "start_time": "2024-06-14T11:31:08.168949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_cells = 4_000\n",
    "n_genes = 20_000\n",
    "\n",
    "raw_counts = np.random.randint(low = 0, high = 100, size = (n_cells, n_genes))\n",
    "raw_counts_sparse = sp.sparse.csr_matrix(raw_counts)\n",
    "perturbations = (\n",
    "    [\"GENETIC:MYC\"] * 1000\n",
    "    + [\"GENETIC:AKT\"] * 1000\n",
    "    + [\"GENETIC:PD1\"] * 1000\n",
    "    + [None] * 1000\n",
    ")\n",
    "sex = np.random.choice([\"male\", \"female\"], size = n_cells)\n",
    "cell_type = np.random.choice([\"cell_type1\", \"cell_type2\"], size = n_cells)\n",
    "\n",
    "anndata = ad.AnnData(raw_counts_sparse, obs = pd.DataFrame({\"perturbation\": perturbations, \"sex\": sex, \"cell_type\": cell_type}))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ab70/Library/Caches/pypoetry/virtualenvs/perturbench-mht8PlFG-py3.9/lib/python3.9/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:11.066687Z",
     "start_time": "2024-06-14T11:31:11.063602Z"
    }
   },
   "source": "perturbation_dataset = PerturbationDataset(anndata, \"perturbation\", [\"sex\", \"cell_type\"])",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:11.074039Z",
     "start_time": "2024-06-14T11:31:11.067353Z"
    }
   },
   "cell_type": "code",
   "source": "perturbation_dataset.anndata()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 4000 × 20000\n",
       "    obs: 'sex', 'cell_type', 'perturbation'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T16:31:52.976370Z",
     "start_time": "2024-06-13T16:31:52.960348Z"
    }
   },
   "source": [
    "## 2. Declare a scenario\n",
    "\n",
    "How should we define train-test splits? First step will be to create a scenario."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:11.076662Z",
     "start_time": "2024-06-14T11:31:11.075189Z"
    }
   },
   "cell_type": "code",
   "source": "my_favourite_scenario = RandomSplitScenario()",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And how let's create a train and test dataset using this scenario!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:31:14.976846Z",
     "start_time": "2024-06-14T11:31:14.497785Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, test_dataset = my_favourite_scenario.split(perturbation_dataset)",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Run model training\n",
    "\n",
    "Now let's pick the model we would like to benchmark! We'll use a simple random example (shouldn't perform well!)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random_model_1 = RandomModel(device=device)\n",
    "random_model_2 = RandomModel(device=device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Choose our metrics\n",
    "\n",
    "Now we need to select which metrics we would like to use! Let's do a simple average difference between ground truth and prediction."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "my_favourite_metric = AverageDifferenceMetric()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Finally, register this all with a benchmark object\n",
    "\n",
    "Now, let's register this all and check what's going to be run!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "benchmark = PerturbationBenchmark()\n",
    "\n",
    "# register our datasets\n",
    "\n",
    "benchmark.add_dataset(perturbation_dataset)\n",
    "\n",
    "# register scenarios\n",
    "\n",
    "benchmark.add_scenario(my_favourite_scenario)\n",
    "\n",
    "# register models\n",
    "\n",
    "for model in [random_model_1, random_model_2]:\n",
    "    benchmark.add_model(model)\n",
    "    \n",
    "# register metrics\n",
    "\n",
    "for metric in [my_favourite_metric]:\n",
    "    benchmark.add_metric(metric)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check what's going to be run!"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "benchmark.plan()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, we're happy with that plan! Now let's run the benchmarking :)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "benchmark.run()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally let's view the results as a table!"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "benchmark.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
